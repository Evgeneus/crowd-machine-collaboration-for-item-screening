import numpy as np
from scipy.stats import beta

from src.screening_algorithms.helpers.utils import Metrics


class MachineEnsemble(Metrics):

    def __init__(self, params):
        self.filters_num = params['filters_num']
        self.items_num = params['items_num']
        self.ground_truth = params['ground_truth']
        self.lr = params['lr']
        self.corr = params['corr']
        self.machine_tests = params['machine_tests']
        self.select_conf = params['select_conf']
        self.machines_num = params['machines_num']
        self.machine_acc_range = params['machine_acc_range']
        # metrics to be computed
        self.loss = None
        self.recall = None
        self.precision = None
        self.f_beta = None
        self.price_per_paper = None

    def run(self):
        # check number of machines passed tests
        while True:
            machines_accuracy, estimated_acc = self._get_machines()
            if len(estimated_acc) > 2:
                break

        votes_list = [[] for _ in range(self.items_num * self.filters_num)]

        # generate votes for the first machine
        first_machine_acc = machines_accuracy[0]
        for item_index in range(self.items_num):
            for filter_index in range(self.filters_num):
                gt = self.ground_truth[item_index * self.filters_num + filter_index]  # can be either 0 or 1
                if np.random.binomial(1, first_machine_acc):
                    vote = gt
                else:
                    vote = 1 - gt
                votes_list[item_index * self.filters_num + filter_index].append(vote)

        # generate votes for the rest machines
        rest_machine_acc = machines_accuracy[1:]
        for item_index in range(self.items_num):
            for filter_index in range(self.filters_num):
                gt = self.ground_truth[item_index * self.filters_num + filter_index]  # can be either 0 or 1
                vote_prev = votes_list[item_index * self.filters_num + filter_index][-1]
                for machine_acc in rest_machine_acc:
                    vote = self._generate_vote(gt, machine_acc, vote_prev)
                    votes_list[item_index * self.filters_num + filter_index].append(vote)

        # ensemble votes for each filter and item
        ensembled_votes = self._naive_bayes(votes_list, estimated_acc)

        items_labels = self._classify_items(ensembled_votes)
        metrics = self.compute_metrics(items_labels, self.ground_truth, self.lr, self.filters_num)
        self.loss = metrics[0]
        self.recall = metrics[1]
        self.precision = metrics[2]
        self.f_beta = metrics[3]
        return self.loss, self.recall, self.precision, self.f_beta, ensembled_votes, \
               [machines_accuracy, estimated_acc, self.ground_truth_tests, self.machine_test_votes, votes_list]

    def _get_machines(self):
        test_votes = [[] for _ in range(self.machines_num)]

        # generate accuracy of machines
        machines_acc = np.random.uniform(self.machine_acc_range[0], self.machine_acc_range[1], self.machines_num)
        # assign max acc for the first machine as the error accumulates with machine number increases
        first_machine_acc = max(machines_acc)
        machines_acc[np.where(machines_acc == first_machine_acc)] = machines_acc[0]
        machines_acc[0] = first_machine_acc

        self.ground_truth_tests = [0]*(self.machine_tests//2) + [1]*(self.machine_tests//2)

        # set votes on tests that are generated by first machine
        for gt in self.ground_truth_tests:
            if np.random.binomial(1, first_machine_acc):
                test_votes[0].append(gt)
            else:
                test_votes[0].append(1-gt)

        # generate votes for the rest machines to be tested
        for prev_machine_id, acc in enumerate(machines_acc[1:]):
            for i, gt in enumerate(self.ground_truth_tests):
                prev_machine_vote = test_votes[prev_machine_id][i]
                if np.random.binomial(1, self.corr):
                    if gt != prev_machine_vote:
                        vote = prev_machine_vote
                    else:
                        vote = gt if np.random.binomial(1, acc) else 1 - gt
                else:
                    vote = gt if np.random.binomial(1, acc) else 1 - gt
                test_votes[prev_machine_id + 1].append(vote)

        selected_machines_acc = []
        estimated_acc = []
        self.machine_test_votes = []
        for machine_votes, acc in zip(test_votes, machines_acc):
            correct_votes_num = sum([1 if i == j else 0 for i, j in zip(self.ground_truth_tests, machine_votes)])
            conf = beta.sf(0.5, correct_votes_num + 1, self.machine_tests - correct_votes_num + 1)
            if conf > self.select_conf:
                selected_machines_acc.append(acc)
                m_acc = correct_votes_num / self.machine_tests
                # to avoid border cases
                if m_acc > 0.95:
                    m_acc = 0.95
                estimated_acc.append(m_acc)
                self.machine_test_votes.append(machine_votes)

        # # check number of machines passed tests
        # # add at least one machine passed tests (accuracy in self.machine_acc_range)
        # if len(selected_machines_acc) == 0:
        #     # TODO: No machines selected, i.e., zero machines passed tests  => need more accurate classifiers
        #     acc = np.random.uniform(0.5, self.machine_acc_range[1])
        #     while True:
        #         test_votes[0] = []
        #         # set votes on tests that are generated by first machine
        #         for gt in self.ground_truth_tests:
        #             if np.random.binomial(1, acc):
        #                 test_votes[0].append(gt)
        #             else:
        #                 test_votes[0].append(1 - gt)
        #         correct_votes_num = sum([1 if i == j else 0 for i, j in zip(self.ground_truth_tests, test_votes[0])])
        #         conf = beta.sf(0.5, correct_votes_num + 1, self.machine_tests - correct_votes_num + 1)
        #         if conf > self.select_conf:
        #             m_acc = correct_votes_num / self.machine_tests
        #             # to avoid border cases
        #             if m_acc > 0.95:
        #                 m_acc = 0.95
        #             selected_machines_acc.append(acc)
        #             estimated_acc.append(m_acc)
        #             self.machine_test_votes.append(test_votes[0])
        #             break

        return selected_machines_acc, estimated_acc

    def _generate_vote(self, gt, acc, vote_prev):
        if np.random.binomial(1, self.corr):
            if vote_prev != gt:
                vote = vote_prev
            else:
                if np.random.binomial(1, acc):
                    vote = gt
                else:
                    vote = 1 - gt
        else:
            if np.random.binomial(1, acc):
                vote = gt
            else:
                vote = 1 - gt
        return vote

    # fuse votes via weighted majority voting
    # output_data: probabilities to be negatives for each filter and item
    def _naive_bayes(self, votes_list, estimated_acc):
        probs_list = [None] * self.filters_num * self.items_num
        for filter_index in range(self.filters_num):
            filter_machines_acc = estimated_acc
            for item_index in range(self.items_num):
                like_true_val = 1  # assume true value is positive
                a, b = 1., 1.  # constituents of baysian formula, prior is uniform dist.
                # a responds for positives, b - for negatives
                for vote, acc in zip(votes_list[item_index * self.filters_num + filter_index], filter_machines_acc):
                    if vote == like_true_val:
                        a *= acc
                        b *= 1 - acc
                    else:
                        a *= 1 - acc
                        b *= acc
                probs_list[item_index * self.filters_num + filter_index] = b / (a + b)
        return probs_list

    def _classify_items(self, ensembled_votes):
        items_labels = []
        neg_thr = 0.99  # threshold to classify as a positive
        for item_index in range(self.items_num):
            prob_filters_not_apply = 1.
            for filter_index in range(self.filters_num):
                prob_filters_not_apply *= ensembled_votes[item_index * self.filters_num + filter_index]
            prob_item_out = 1. - prob_filters_not_apply

            # classify item
            if prob_item_out > neg_thr:
                items_labels.append(0)
            else:
                items_labels.append(1)
        return items_labels
